{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.표준 토큰화\n",
        "\n",
        "자연어 처리에 사용되는 대표적인 파이썬 패키지(영문)->NLTK가 있다. 해당 패키지는 Copurs,Creat Token, 형태소 분석, 품사 태깅 등을 제공한다"
      ],
      "metadata": {
        "id": "I1Riozb_rUh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 표준 토큰화\n",
        "\n",
        "표준 토큰화 중 하나인 Treebank 표준 토큰화를 사용하는 방법"
      ],
      "metadata": {
        "id": "xoKMR06Irurc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer #표준화 단어 tokenizer 불러옴\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Model-based RL don't need a value function for the policy\"\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZX_uWZprv5S",
        "outputId": "372b70c0-c8ee-46b9-bedf-8f855ab7faee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 토큰화 라이브러리\n",
        "\n",
        "Treebank 토큰화 이외에도 NLTk 패키지에는 여러 종류의 토큰화 패키지가 있습니다. 예를 들면 아래와 같은 word_tokenizer"
      ],
      "metadata": {
        "id": "Kn9hupelu2Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ho8Vy5LvUEu",
        "outputId": "c0934859-ac84-4e92-a96e-86061277ec23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99ToP1MZvYhW",
        "outputId": "c6501622-0fdc-4e74-92c7-f7c8c31fe5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(text))\n"
      ],
      "metadata": {
        "id": "xE5EgqRPwE4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e8339e-d31b-4342-e872-c765f6e5bc3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정제 및 추출"
      ],
      "metadata": {
        "id": "FMggdJPFSEmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.어간 추출 및 표제어 추출\n",
        "단어의 형태소 level에서 분석을 하게 되면 다른 품사 또는 다른 시제의 단어라고 해도 같은 형태로 토큰화를 할 수 있다. 어간 추출 하는 방법"
      ],
      "metadata": {
        "id": "fVK5o0_KwrKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 어간 추출(Stemmer) VS 표제어 추출(Lemmatizer)\n",
        "\n",
        "대표적인 어간 추출(stemming) 기법인 Porter 및 Lancaster 추출 패키지를 불러오고, 이를 활용하는 방법 결과를 보고 어간 추출 기반 기법의 문제점 파악"
      ],
      "metadata": {
        "id": "-zPUVKAnw5Bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "내가 생각하는 문제점? : 어간을 제대로 추출해내지 못함\n",
        "LancasterStemmer는 그래도 ate를 제외하곤 제대로 추출하였지만\n",
        "PorterStemmer는 그러하지 못함"
      ],
      "metadata": {
        "id": "1LRBBphdzUd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "어간 추출"
      ],
      "metadata": {
        "id": "-QCRSbK9SQdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "stem1 = PorterStemmer()\n",
        "stem2 = LancasterStemmer()\n",
        "words = [\"eat\",\"ate\",\"eaten\",\"eating\"]\n",
        "print(\"PorterStemmer :\", [stem1.stem(w) for w in words]) #poterstemmer을 w에 변수를 poterstemmer를 이용하고 반복문을 이용하여 리스트에 있는 단어를 하나씩 꺼내서 추출\n",
        "print(\"LancasterStemmer :\", [stem2.stem(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA6wjxs_xL85",
        "outputId": "aa8fbacb-540a-4616-e45a-77a23f563226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PorterStemmer : ['eat', 'ate', 'eaten', 'eat']\n",
            "LancasterStemmer : ['eat', 'at', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "표제어 추출"
      ],
      "metadata": {
        "id": "f3zuKtcBSSUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5O85eBySS7X",
        "outputId": "e6a57098-ec2c-462b-f98f-524c672959e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGFvOcKgWFRn",
        "outputId": "8285018b-2c1d-43f1-8675-1887d902d9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordNetLemmatizer\n",
        "lemm = WordNetLemmatizer()\n",
        "words = [\"eat\",\"ate\",\"eaten\",\"eating\"]\n",
        "print(\"WordNetLemmatizer :\", [lemm.lemmatize(w, pos =\"v\")for w in words]) #pos를 통해 동사를 품사태깅 한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfz9bMtHS30M",
        "outputId": "5238d7a8-8db3-415f-f107-93124301ea93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNetLemmatizer : ['eat', 'eat', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.불용어 제거"
      ],
      "metadata": {
        "id": "fsC5-VlbWa4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어에서의 불용어 중 예시 몇 가지를 확인 하는 방법. 마지막 줄의 숫자를 증가시키면 볼 수 있는 단어의 수가 증가한다. 마찬가지로 stopword 단어 데이터를 받기 위한 작업이 필요함"
      ],
      "metadata": {
        "id": "1KwmQ-YCWduz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english')[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewzo2BE7WqLO",
        "outputId": "e06f6418-82ea-435e-82e0-66d27802dba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M6ZDHSHW320",
        "outputId": "2c295180-a2d9-4bf1-8635-785bd2135d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.6/465.6 KB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from Konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from Konlpy) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->Konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->Konlpy) (3.0.9)\n",
            "Installing collected packages: JPype1, Konlpy\n",
            "Successfully installed JPype1-1.4.1 Konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "input_sentence = \"We should all study hard for the exam\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(input_sentence)\n",
        "result =[]\n",
        "for w in word_tokens: # <- 토큰화 결과에 대해서\n",
        "  if w not in stop_words:   #<- we부터 해서 쭉쭉 빼서 오는데 일단 we를 stop_words에 있나 구분하고 없으면 결과에 추가한다.\n",
        "    result.append(w)\n",
        "print(word_tokens)\n",
        "print(result)\n",
        "\n",
        "# 각 단어마다 불용어 목록에 있으면 포함 하고 없으면 포함하지 말자는 코드"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgA_qWjAXmOM",
        "outputId": "1356758d-9c14-4089-fd3f-27df38f65bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'should', 'all', 'study', 'hard', 'for', 'the', 'exam']\n",
            "['We', 'study', 'hard', 'exam']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.정수 인코딩 및 sorting\n",
        "4.1 Enumerate"
      ],
      "metadata": {
        "id": "kiX3LfRLZq5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"English\",'math','Science']\n",
        "for n, name in enumerate(list): #n은 순서 name은 값 Enumerate를 사용하면 키와 값을 한번에 볼수 있다.\n",
        "  print(\"Course : {}, Number : {}\".format(name, n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiT6-VDZaYcq",
        "outputId": "49c20151-a551-4bb8-af5d-d15b92a4c89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Course : English, Number : 0\n",
            "Course : math, Number : 1\n",
            "Course : Science, Number : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 정수 인코딩 및 High-frequency Sorting"
      ],
      "metadata": {
        "id": "AGm85_9BcZKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={'apple':2, 'july' : 6, 'piano' : 4} #bow(단어마다 문장 빈도수 apple은 2번 나왔고 july은 6번 나왔고 ... 다는 말)\n",
        "vocab_sort = sorted(vocab.items(), key = lambda x:x[1],reverse = True) #lambda는 임수 함수 -> 한번만 사용하고 만다 x:x[1]을 리턴한다 reverse= 내림차순 (큰->작)\n",
        "print(vocab_sort)\n",
        "word2inx = {word[0] : index + 1 for index, word in enumerate(vocab_sort)} #word[0] -> vocab에서 문자를 의미 word[1]을 하면 값을 의미 함 그 후 인덱스를 하나씩 증가하면서 \n",
        "print(word2inx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKAGaM2dce3z",
        "outputId": "ff9efaf1-afbc-473a-dd4c-ad5d6255ced6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('july', 6), ('piano', 4), ('apple', 2)]\n",
            "{'july': 1, 'piano': 2, 'apple': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW로 만들어진 토큰화의 결과를 가장 높은 빈도수부터 재정렬하고, 이를 통해 정수 인코딩을 하는 과정"
      ],
      "metadata": {
        "id": "v4mo04-DfD5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.inputtransformer2 import tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Model-based RL don't need a value function for the policy,\"\\\n",
        "       \"but some of Model-based RL algorithms do have a value function. \"\n",
        "token_text = tokenizer.tokenize(text) #여기 까지 토큰 리스트를 생성\n",
        "word2inx = {} #dic값을 저장\n",
        "Bow = [] #list값을 저장\n",
        "for word in token_text: #토큰화 된 단어 마다\n",
        "  if word not in word2inx.keys(): # 만약 word2inx dic에 키값에 단어가 없으면 순서대로 왔다 했을 때 model이 오니깐 키는 없음\n",
        "      word2inx[word] = len(word2inx) #그럼 word2inx[modle] = len(wordinx) -> 없으니깐 0이 온다.\n",
        "      Bow.insert(len(word2inx)-1,1) #그러고 Bow에 (-1,1)에 추가해라 그럼 modle은 (0,1)이 되고 토큰화 된 다음 단어가 오면 (1,1)...(n,1)이 된다.\n",
        "  else:\n",
        "    inx = word2inx.get(word) #model이 다시 왔을 때 word2inx에 model이 이미 키값을 부여받고 존재 하니깐 여기로 오고 0값을 부여 받았으니 0값을 가져와라\n",
        "    Bow[inx] +=1   #그 후 bow에 0+1을해서 증가 시키고 model은 2이 된다 또 model이 오면 3이되고 \n",
        "print(word2inx) #-> 그냥 토큰화 된 것들을 0~n까지 번호를 부여된걸 보여줘라\n",
        "print(Bow) #-> 빈도수를 알려줘라\n",
        "\n",
        "## 조금 더 반복 학습과 이해가 필요함  1try"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSqB81jffMWv",
        "outputId": "56cb2074-96bf-4f4f-ecec-886c1d90cfa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Model-based': 0, 'RL': 1, 'do': 2, \"n't\": 3, 'need': 4, 'a': 5, 'value': 6, 'function': 7, 'for': 8, 'the': 9, 'policy': 10, ',': 11, 'but': 12, 'some': 13, 'of': 14, 'algorithms': 15, 'have': 16, '.': 17}\n",
            "[2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기 까지 순서는 문장->토큰화->정제 및 추출-> 인코딩- 정수 인코딩"
      ],
      "metadata": {
        "id": "7hjmUaMIpaBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.유사도 분석\n",
        "5.1 코사인 유사도"
      ],
      "metadata": {
        "id": "0lBHYCypkFt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cos_sim(A,B):\n",
        "    return np.dot(A,B) / (np.linalg.norm(A)*np.linalg.norm(B)) #np.dot-> a,b와 내적 / np.linalg.norm ->백터의 크기\n",
        "a=[1,0,0,1]\n",
        "b=[0,1,1,0]\n",
        "c=[1,1,1,1]\n",
        "print(\"A와B의 유사도 : \", cos_sim(a,b),'\\n',\"B와C의 유사도 : \",cos_sim(b,c),'\\n',\"A와C의 유사도 : \",cos_sim(a,c))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOAK5NGGkUeP",
        "outputId": "b12c8832-1c09-4e85-c455-6b4b921d11ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A와B의 유사도 :  0.0 \n",
            " B와C의 유사도 :  0.7071067811865475 \n",
            " A와C의 유사도 :  0.7071067811865475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2 레반슈타인 거리"
      ],
      "metadata": {
        "id": "zPzNqSwEq0oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def leven (text1,text2) :\n",
        "  len1 =len(text1) + 1 \n",
        "  len2 =len(text2) + 1 \n",
        "  sim_array = np.zeros((len1, len2))\n",
        "  sim_array[:,0] = np.linspace(0, len1-1,len1) #테이블을 생성하고 그 행 테이블에 1씩 차이나게 넣어줘라\n",
        "  sim_array[0,:] = np.linspace(0, len2-1,len2) #테이블을 생성하고 그 열 테이블에 1씩 차이나게 넣어줘라\n",
        "  for i in range(1,len1):\n",
        "    for j in range(1,len2):\n",
        "      add_char = sim_array[i-1,j] +1 #바로 위에서 +1씩 증가하게 해라 ->레반슈타인 거리 규칙 \n",
        "      sub_char = sim_array[i,j-1] +1 #바로 옆에서 +1씩 증가하게 해라 ->레반슈타인 거리 규칙\n",
        "      if text1[i-1] == text2[j-1]: # 수정이 되지 않았다면 대각선의 값과 동일하다\n",
        "        mod_char = sim_array[i-1,j-1]\n",
        "      else:\n",
        "        mod_char = sim_array[i-1,j-1] + 1 #수정되었다면 대각선 값 +1을 해라\n",
        "\n",
        "      sim_array[i,j] = min([add_char, sub_char, mod_char]) #상태가 수정(더하거나,빼거나,변경)할 때, 최소값을 골라서 return 해라\n",
        "      print(f'i == {i}, j == {j} 일때')\n",
        "      print(f'add_char = {add_char}')\n",
        "      print(f'sub_char = {sub_char}')\n",
        "      print(f'mod_char = {mod_char}')\n",
        "      print(sim_array)\n",
        "      print(\"-----------------------------\")\n",
        "\n",
        "  return sim_array #(-1,-1)은 가장 오른쪽 끝을 나타낸다.\n",
        "\n",
        "print(leven ('데이터마이닝','데이타마닝'))\n",
        "\n",
        "## 값이 이상함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM1aB7MDq2rJ",
        "outputId": "3dc51fdf-becf-4706-d5c9-42a6d7483e8a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i == 1, j == 1 일때\n",
            "add_char = 2.0\n",
            "sub_char = 2.0\n",
            "mod_char = 0.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [2. 0. 0. 0. 0. 0.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 1, j == 2 일때\n",
            "add_char = 3.0\n",
            "sub_char = 1.0\n",
            "mod_char = 2.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 0. 0. 0.]\n",
            " [2. 0. 0. 0. 0. 0.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 1, j == 3 일때\n",
            "add_char = 4.0\n",
            "sub_char = 2.0\n",
            "mod_char = 3.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 0. 0.]\n",
            " [2. 0. 0. 0. 0. 0.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 1, j == 4 일때\n",
            "add_char = 5.0\n",
            "sub_char = 3.0\n",
            "mod_char = 4.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 0.]\n",
            " [2. 0. 0. 0. 0. 0.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 1, j == 5 일때\n",
            "add_char = 6.0\n",
            "sub_char = 4.0\n",
            "mod_char = 5.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 0. 0. 0. 0. 0.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 2, j == 1 일때\n",
            "add_char = 1.0\n",
            "sub_char = 3.0\n",
            "mod_char = 2.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 0. 0. 0.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 2, j == 2 일때\n",
            "add_char = 2.0\n",
            "sub_char = 2.0\n",
            "mod_char = 0.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 0. 0. 0.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 2, j == 3 일때\n",
            "add_char = 3.0\n",
            "sub_char = 1.0\n",
            "mod_char = 2.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 0. 0.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 2, j == 4 일때\n",
            "add_char = 4.0\n",
            "sub_char = 2.0\n",
            "mod_char = 3.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 0.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 2, j == 5 일때\n",
            "add_char = 5.0\n",
            "sub_char = 3.0\n",
            "mod_char = 4.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 3, j == 1 일때\n",
            "add_char = 2.0\n",
            "sub_char = 4.0\n",
            "mod_char = 3.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 3, j == 2 일때\n",
            "add_char = 1.0\n",
            "sub_char = 3.0\n",
            "mod_char = 2.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 3, j == 3 일때\n",
            "add_char = 2.0\n",
            "sub_char = 2.0\n",
            "mod_char = 1.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 3, j == 4 일때\n",
            "add_char = 3.0\n",
            "sub_char = 2.0\n",
            "mod_char = 2.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 3, j == 5 일때\n",
            "add_char = 4.0\n",
            "sub_char = 3.0\n",
            "mod_char = 3.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 4, j == 1 일때\n",
            "add_char = 3.0\n",
            "sub_char = 5.0\n",
            "mod_char = 4.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 4, j == 2 일때\n",
            "add_char = 2.0\n",
            "sub_char = 4.0\n",
            "mod_char = 3.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 4, j == 3 일때\n",
            "add_char = 2.0\n",
            "sub_char = 3.0\n",
            "mod_char = 2.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 4, j == 4 일때\n",
            "add_char = 3.0\n",
            "sub_char = 3.0\n",
            "mod_char = 1.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 4, j == 5 일때\n",
            "add_char = 4.0\n",
            "sub_char = 2.0\n",
            "mod_char = 3.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 5, j == 1 일때\n",
            "add_char = 4.0\n",
            "sub_char = 6.0\n",
            "mod_char = 5.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 5, j == 2 일때\n",
            "add_char = 3.0\n",
            "sub_char = 5.0\n",
            "mod_char = 3.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 3. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 5, j == 3 일때\n",
            "add_char = 3.0\n",
            "sub_char = 4.0\n",
            "mod_char = 3.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 3. 3. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 5, j == 4 일때\n",
            "add_char = 2.0\n",
            "sub_char = 4.0\n",
            "mod_char = 3.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 3. 3. 2. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 5, j == 5 일때\n",
            "add_char = 3.0\n",
            "sub_char = 3.0\n",
            "mod_char = 2.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 3. 3. 2. 2.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 6, j == 1 일때\n",
            "add_char = 5.0\n",
            "sub_char = 7.0\n",
            "mod_char = 6.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 3. 3. 2. 2.]\n",
            " [6. 5. 0. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 6, j == 2 일때\n",
            "add_char = 4.0\n",
            "sub_char = 6.0\n",
            "mod_char = 5.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 3. 3. 2. 2.]\n",
            " [6. 5. 4. 0. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 6, j == 3 일때\n",
            "add_char = 4.0\n",
            "sub_char = 5.0\n",
            "mod_char = 4.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 3. 3. 2. 2.]\n",
            " [6. 5. 4. 4. 0. 0.]]\n",
            "-----------------------------\n",
            "i == 6, j == 4 일때\n",
            "add_char = 3.0\n",
            "sub_char = 5.0\n",
            "mod_char = 4.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 3. 3. 2. 2.]\n",
            " [6. 5. 4. 4. 3. 0.]]\n",
            "-----------------------------\n",
            "i == 6, j == 5 일때\n",
            "add_char = 3.0\n",
            "sub_char = 4.0\n",
            "mod_char = 2.0\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 3. 3. 2. 2.]\n",
            " [6. 5. 4. 4. 3. 2.]]\n",
            "-----------------------------\n",
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 1. 0. 1. 2. 3.]\n",
            " [3. 2. 1. 1. 2. 3.]\n",
            " [4. 3. 2. 2. 1. 2.]\n",
            " [5. 4. 3. 3. 2. 2.]\n",
            " [6. 5. 4. 4. 3. 2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.Word2Vec- CBoW, Skip-Gram\n",
        "CBoW => 주변 단어들을 보고 중심 단어를 유추하는 것\n",
        "Skip-Gram => 중심 단어를 보고 주변 단어들을 유추하는 것"
      ],
      "metadata": {
        "id": "C9hxzFEfPGrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.1 cBow와 Skipgram을 위한 전처리 복습 "
      ],
      "metadata": {
        "id": "6H0ESeAGPc5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "data =  pd.read_csv('sample_submission.csv') #pd의 함수를 이용해 sample_data 불러오기\n",
        "print('Missing Values : ', data.isnull().sum()) #data 안에 있는 null값이 있으면 그것을 더해서 보여줘\n",
        "data = data.dropna().reset_index(drop=True) # data안에 비어있는 값들은 정리해라\n",
        "merge_data = ' '.join(str(data.iloc[i,0])for i in range(100)) #100차원으로 쭉 나열해라\n",
        "print('Total word count : ', len(merge_data)) # 그래서 그 나열한 data들의 길이를 구해라 pandas iloc 참고\n",
        "\n",
        "print(merge_data[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU-L6gJSPhEC",
        "outputId": "05c5cfcc-bd9c-4108-a8f3-64a05c0f304e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values :  index    0\n",
            "label    0\n",
            "dtype: int64\n",
            "Total word count :  699\n",
            "174304 174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
        "token_text = tokenizer.tokenize(merge_data) #text는 merge_data에 있는 값을 가져오고 그것을 토큰화 해라\n",
        "\n",
        "stop_words = set(stopwords.words('english')) #불용어 제거는 불용어제거 메소드에 있는 english 목록을 가져와\n",
        "token_stop_text = []\n",
        "for w in token_text:\n",
        "  if w not in stop_words:\n",
        "    token_stop_text.append(w) \n",
        "print('After cleaning: ', len(token_stop_text))\n",
        "print(token_stop_text[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBsbN1qQcrKp",
        "outputId": "5ca53c6c-b5a4-4641-e8d8-f68c3a47f51c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After cleaning:  100\n",
            "['174304', '174305', '174306', '174307', '174308', '174309', '174310', '174311', '174312', '174313']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2inx = {}\n",
        "Bow = []\n",
        "for word in token_stop_text:\n",
        "  if word not in word2inx.keys():\n",
        "    Bow.insert(len(word2inx)-1,1)\n",
        "  else :\n",
        "    inx = word2inx.get(word)\n",
        "    Bow[inx] += 1\n",
        "print('Unique Words Count :', len(Bow))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qSy9MYxgXR-",
        "outputId": "4cec6a64-31f3-4e22-b70d-911968d104c8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Words Count : 100\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 까지 \n",
        "전처리 불용어 작업"
      ],
      "metadata": {
        "id": "eS1hZIb_pvS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.2 nltk 내장 함수를 이용한 cbow 학습"
      ],
      "metadata": {
        "id": "KhWIZ2fHp1yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqNERlNPs1Lu",
        "outputId": "374c729a-0a97-420d-e07e-8a6526a310d5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "token_stop_text = np.reshape(np.array(token_stop_text),[-1,1]) #row쪽으로 쌓이도록 reshape\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(vector_size = 100, window = 5, min_count = 2, sg =0)\n",
        "model.bulid_vocab(token_stop_text)\n",
        "model.train(token_stop_text, total_examples= model.corpus_count, epochs = 30, report_delay = 1)\n",
        "vocabs = model.wv.key_to_index.keys() #어떤 단어가 어떻게 임베딩 되어있는지 확인하는 방법\n",
        "word_vec_list = [model.wv[i] for i in vocabs]\n",
        "# sg : skip-gram, 1 is SG, 0 is CBow\n",
        "# vector_size : embedded vector size\n",
        "# window : context window size\n",
        "# min_count : do not apply word2Vec to sparse words\n",
        "\n",
        "\n",
        "#나중에 강의자료에  csv파일을 받았을 때 사용해보기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "Dgv9J--ip5Da",
        "outputId": "70b3c15c-c8d6-461e-e12c-bc01eead791b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-d32e407a5f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtoken_stop_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_stop_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbulid_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_stop_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_stop_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'vector_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.3 pca를 통해 학습 모델화 시각화"
      ],
      "metadata": {
        "id": "i-zhm1qCtzvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca  = PCA(n_components = 2)\n",
        "pcafit = pca.fit_transform(word_vec_list)\n",
        "x = pcafit[0:50:,0]\n",
        "y = pcafit[0:50,1]\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x,y,marker = 'o')\n",
        "for i, v in enumerate(vocabs):\n",
        "  if i <=49:\n",
        "    plt.annotate(v, xy = (x[i], y[i]))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "vp7abEyos5GQ",
        "outputId": "a4bd08a1-f9da-43e4-819a-a3e17b4d021e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-8b9f6eb820a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpca\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpcafit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vec_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpcafit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpcafit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_vec_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. SGNS : SkipGram with Negative Sampling\n",
        "7.1 skipGram 적용 Dataset 구성\n",
        "\n",
        "토큰화된 결과만 필요했던 CBOW 및 SkipGram과 달리, SGNS는 두 단어의 인접 여부가 labelling을  torch의 전처리 도구 활용\n",
        "기본적인 토큰화 과정을 거친 후에, skipgram 함수를 이용하여 변환한다."
      ],
      "metadata": {
        "id": "6N0V7vHSup0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "data = pd.read_csv('~~~')\n",
        "print('Missing data : ',data.isnull().sum())\n",
        "data = data.dropna().reset_index(drop=True)\n",
        "merge_data = ''.join(str(data.iloc[i,0])for i in range(30))\n",
        "print('Total word count : ' , len(merge_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "XhtyTbgsv5S7",
        "outputId": "e9638c4d-f8a2-42ca-9e6d-78bc58f59541"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-5325f53dbec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~~~'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing data : '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~~~'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import skipgrams\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(token_stop_text)\n",
        "word2idx = tokenizer,word_index\n",
        "encoded = tokenizer.texts_to_sequences(token_stop_text)\n",
        "encoded = np.array(encoded).T\n",
        "from tensorflow.keras.preprocessing.sequences import skipgrams\n",
        "skip_gram = [skipgrams(sample, vocabulary_size = len(word2idx)+1,window_size = 10) for sample in encoded]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "tHWK3KlcwuZj",
        "outputId": "7f60b8c5-4a6f-4f26-a913-f07d6e23fbc5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-3baebabc8c51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_stop_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_stop_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m           seq = text_to_word_sequence(\n\u001b[0m\u001b[1;32m    280\u001b[0m               text, filters=self.filters, lower=self.lower, split=self.split)\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import LongTensor as LT\n",
        "from torch import FloatTensor as FT"
      ],
      "metadata": {
        "id": "iXqpLBXWxWsY"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "SnCYtDFdyu8y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}