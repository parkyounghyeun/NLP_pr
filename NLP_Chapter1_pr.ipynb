{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.표준 토큰화\n",
        "\n",
        "자연어 처리에 사용되는 대표적인 파이썬 패키지(영문)->NLTK가 있다. 해당 패키지는 Copurs,Creat Token, 형태소 분석, 품사 태깅 등을 제공한다"
      ],
      "metadata": {
        "id": "I1Riozb_rUh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 표준 토큰화\n",
        "\n",
        "표준 토큰화 중 하나인 Treebank 표준 토큰화를 사용하는 방법"
      ],
      "metadata": {
        "id": "xoKMR06Irurc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer #표준화 단어 tokenizer 불러옴\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Model-based RL don't need a value function for the policy\"\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZX_uWZprv5S",
        "outputId": "372b70c0-c8ee-46b9-bedf-8f855ab7faee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 토큰화 라이브러리\n",
        "\n",
        "Treebank 토큰화 이외에도 NLTk 패키지에는 여러 종류의 토큰화 패키지가 있습니다. 예를 들면 아래와 같은 word_tokenizer"
      ],
      "metadata": {
        "id": "Kn9hupelu2Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ho8Vy5LvUEu",
        "outputId": "c0934859-ac84-4e92-a96e-86061277ec23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99ToP1MZvYhW",
        "outputId": "c6501622-0fdc-4e74-92c7-f7c8c31fe5c3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(text))\n"
      ],
      "metadata": {
        "id": "xE5EgqRPwE4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e8339e-d31b-4342-e872-c765f6e5bc3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정제 및 추출"
      ],
      "metadata": {
        "id": "FMggdJPFSEmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.어간 추출 및 표제어 추출\n",
        "단어의 형태소 level에서 분석을 하게 되면 다른 품사 또는 다른 시제의 단어라고 해도 같은 형태로 토큰화를 할 수 있다. 어간 추출 하는 방법"
      ],
      "metadata": {
        "id": "fVK5o0_KwrKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 어간 추출(Stemmer) VS 표제어 추출(Lemmatizer)\n",
        "\n",
        "대표적인 어간 추출(stemming) 기법인 Porter 및 Lancaster 추출 패키지를 불러오고, 이를 활용하는 방법 결과를 보고 어간 추출 기반 기법의 문제점 파악"
      ],
      "metadata": {
        "id": "-zPUVKAnw5Bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "내가 생각하는 문제점? : 어간을 제대로 추출해내지 못함\n",
        "LancasterStemmer는 그래도 ate를 제외하곤 제대로 추출하였지만\n",
        "PorterStemmer는 그러하지 못함"
      ],
      "metadata": {
        "id": "1LRBBphdzUd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "어간 추출"
      ],
      "metadata": {
        "id": "-QCRSbK9SQdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "stem1 = PorterStemmer()\n",
        "stem2 = LancasterStemmer()\n",
        "words = [\"eat\",\"ate\",\"eaten\",\"eating\"]\n",
        "print(\"PorterStemmer :\", [stem1.stem(w) for w in words]) #poterstemmer을 w에 변수를 poterstemmer를 이용하고 반복문을 이용하여 리스트에 있는 단어를 하나씩 꺼내서 추출\n",
        "print(\"LancasterStemmer :\", [stem2.stem(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA6wjxs_xL85",
        "outputId": "aa8fbacb-540a-4616-e45a-77a23f563226"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PorterStemmer : ['eat', 'ate', 'eaten', 'eat']\n",
            "LancasterStemmer : ['eat', 'at', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "표제어 추출"
      ],
      "metadata": {
        "id": "f3zuKtcBSSUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5O85eBySS7X",
        "outputId": "e6a57098-ec2c-462b-f98f-524c672959e5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGFvOcKgWFRn",
        "outputId": "8285018b-2c1d-43f1-8675-1887d902d9c2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordNetLemmatizer\n",
        "lemm = WordNetLemmatizer()\n",
        "words = [\"eat\",\"ate\",\"eaten\",\"eating\"]\n",
        "print(\"WordNetLemmatizer :\", [lemm.lemmatize(w, pos =\"v\")for w in words]) #pos를 통해 동사를 품사태깅 한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfz9bMtHS30M",
        "outputId": "5238d7a8-8db3-415f-f107-93124301ea93"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNetLemmatizer : ['eat', 'eat', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.불용어 제거"
      ],
      "metadata": {
        "id": "fsC5-VlbWa4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어에서의 불용어 중 예시 몇 가지를 확인 하는 방법. 마지막 줄의 숫자를 증가시키면 볼 수 있는 단어의 수가 증가한다. 마찬가지로 stopword 단어 데이터를 받기 위한 작업이 필요함"
      ],
      "metadata": {
        "id": "1KwmQ-YCWduz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english')[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewzo2BE7WqLO",
        "outputId": "e06f6418-82ea-435e-82e0-66d27802dba1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M6ZDHSHW320",
        "outputId": "2c295180-a2d9-4bf1-8635-785bd2135d09"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.6/465.6 KB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from Konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from Konlpy) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->Konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->Konlpy) (3.0.9)\n",
            "Installing collected packages: JPype1, Konlpy\n",
            "Successfully installed JPype1-1.4.1 Konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "input_sentence = \"We should all study hard for the exam\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(input_sentence)\n",
        "result =[]\n",
        "for w in word_tokens: # <- 토큰화 결과에 대해서\n",
        "  if w not in stop_words:   #<- we부터 해서 쭉쭉 빼서 오는데 일단 we를 stop_words에 있나 구분하고 없으면 결과에 추가한다.\n",
        "    result.append(w)\n",
        "print(word_tokens)\n",
        "print(result)\n",
        "\n",
        "# 각 단어마다 불용어 목록에 있으면 포함 하고 없으면 포함하지 말자는 코드"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgA_qWjAXmOM",
        "outputId": "1356758d-9c14-4089-fd3f-27df38f65bc4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'should', 'all', 'study', 'hard', 'for', 'the', 'exam']\n",
            "['We', 'study', 'hard', 'exam']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.정수 인코딩 및 sorting\n",
        "4.1 Enumerate"
      ],
      "metadata": {
        "id": "kiX3LfRLZq5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"English\",'math','Science']\n",
        "for n, name in enumerate(list): #n은 순서 name은 값 Enumerate를 사용하면 키와 값을 한번에 볼수 있다.\n",
        "  print(\"Course : {}, Number : {}\".format(name, n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiT6-VDZaYcq",
        "outputId": "49c20151-a551-4bb8-af5d-d15b92a4c89f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Course : English, Number : 0\n",
            "Course : math, Number : 1\n",
            "Course : Science, Number : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 정수 인코딩 및 High-frequency Sorting"
      ],
      "metadata": {
        "id": "AGm85_9BcZKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={'apple':2, 'july' : 6, 'piano' : 4} #bow(단어마다 문장 빈도수 apple은 2번 나왔고 july은 6번 나왔고 ... 다는 말)\n",
        "vocab_sort = sorted(vocab.items(), key = lambda x:x[1],reverse = True) #lambda는 임수 함수 -> 한번만 사용하고 만다 x:x[1]을 리턴한다 reverse= 내림차순 (큰->작)\n",
        "print(vocab_sort)\n",
        "word2inx = {word[0] : index + 1 for index, word in enumerate(vocab_sort)} #word[0] -> vocab에서 문자를 의미 word[1]을 하면 값을 의미 함 그 후 인덱스를 하나씩 증가하면서 \n",
        "print(word2inx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKAGaM2dce3z",
        "outputId": "ff9efaf1-afbc-473a-dd4c-ad5d6255ced6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('july', 6), ('piano', 4), ('apple', 2)]\n",
            "{'july': 1, 'piano': 2, 'apple': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW로 만들어진 토큰화의 결과를 가장 높은 빈도수부터 재정렬하고, 이를 통해 정수 인코딩을 하는 과정"
      ],
      "metadata": {
        "id": "v4mo04-DfD5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.inputtransformer2 import tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Model-based RL don't need a value function for the policy,\"\\\n",
        "       \"but some of Model-based RL algorithms do have a value function. \"\n",
        "token_text = tokenizer.tokenize(text) #여기 까지 토큰 리스트를 생성\n",
        "word2inx = {} #dic값을 저장\n",
        "Bow = [] #list값을 저장\n",
        "for word in token_text: #토큰화 된 단어 마다\n",
        "  if word not in word2inx.keys(): # 만약 word2inx dic에 키값에 단어가 없으면 순서대로 왔다 했을 때 model이 오니깐 키는 없음\n",
        "      word2inx[word] = len(word2inx) #그럼 word2inx[modle] = len(wordinx) -> 없으니깐 0이 온다.\n",
        "      Bow.insert(len(word2inx)-1,1) #그러고 Bow에 (-1,1)에 추가해라 그럼 modle은 (0,1)이 되고 토큰화 된 다음 단어가 오면 (1,1)...(n,1)이 된다.\n",
        "  else:\n",
        "    inx = word2inx.get(word) #model이 다시 왔을 때 word2inx에 model이 이미 키값을 부여받고 존재 하니깐 여기로 오고 0값을 부여 받았으니 0값을 가져와라\n",
        "    Bow[inx] +=1   #그 후 bow에 0+1을해서 증가 시키고 model은 2이 된다 또 model이 오면 3이되고 \n",
        "print(word2inx) #-> 그냥 토큰화 된 것들을 0~n까지 번호를 부여된걸 보여줘라\n",
        "print(Bow) #-> 빈도수를 알려줘라\n",
        "\n",
        "## 조금 더 반복 학습과 이해가 필요함  1try"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSqB81jffMWv",
        "outputId": "56cb2074-96bf-4f4f-ecec-886c1d90cfa4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Model-based': 0, 'RL': 1, 'do': 2, \"n't\": 3, 'need': 4, 'a': 5, 'value': 6, 'function': 7, 'for': 8, 'the': 9, 'policy': 10, ',': 11, 'but': 12, 'some': 13, 'of': 14, 'algorithms': 15, 'have': 16, '.': 17}\n",
            "[2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기 까지 순서는 문장->토큰화->정제 및 추출-> 인코딩- 정수 인코딩"
      ],
      "metadata": {
        "id": "7hjmUaMIpaBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.유사도 분석\n",
        "5.1 코사인 유사도"
      ],
      "metadata": {
        "id": "0lBHYCypkFt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cos_sim(A,B):\n",
        "    return np.dot(A,B) / (np.linalg.norm(A)*np.linalg.norm(B)) #np.dot-> a,b와 내적 / np.linalg.norm ->백터의 크기\n",
        "a=[1,0,0,1]\n",
        "b=[0,1,1,0]\n",
        "c=[1,1,1,1]\n",
        "print(\"A와B의 유사도 : \", cos_sim(a,b),'\\n',\"B와C의 유사도 : \",cos_sim(b,c),'\\n',\"A와C의 유사도 : \",cos_sim(a,c))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOAK5NGGkUeP",
        "outputId": "b12c8832-1c09-4e85-c455-6b4b921d11ae"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A와B의 유사도 :  0.0 \n",
            " B와C의 유사도 :  0.7071067811865475 \n",
            " A와C의 유사도 :  0.7071067811865475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2 레반슈타인 거리"
      ],
      "metadata": {
        "id": "zPzNqSwEq0oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def leven (text1,text2) :  # text1,text2를 생성\n",
        "  len1 =len(text1) + 1  # len1 만들기\n",
        "  len2 =len(text2) + 1  # len2 만들기\n",
        "  sim_array = np.zeros((len1, len2))\n",
        "  sim_array[:,0] = np.linspace(0, len1-1,len1) #테이블을 생성하고 그 행 테이블에 1씩 차이나게 넣어줘라\n",
        "  sim_array[0,:] = np.linspace(0, len2-1,len2) #테이블을 생성하고 그 열 테이블에 1씩 차이나게 넣어줘라\n",
        "  for i in range(1,len1):\n",
        "    for j in range(1,len2):\n",
        "      add_char = sim_array[i,j-1] +1 #바로 위에서 +1씩 증가하게 해라 ->레반슈타인 거리 규칙 \n",
        "      sub_char = sim_array[i-1,j] +1 #바로 옆에서 +1씩 증가하게 해라 ->레반슈타인 거리 규칙\n",
        "      if text1[i-1] == text2[j-1]: # 수정이 되지 않았다면 대각선의 값과 동일하다\n",
        "        mod_char = sim_array[i-1,j-1]\n",
        "      else:\n",
        "        mod_char = sim_array[i-1,j-1] + 1 #수정되었다면 +1을 해라\n",
        "      sim_array[i,j] = min([add_char, sub_char, mod_char]) #상태가 수정(더하거나,빼거나,변경)할 때, 최소값을 골라서 return 해라\n",
        "    return sim_array #(-1,-1)은 가장 오른쪽 끝을 나타낸다.\n",
        "print(leven ('데이터마이닝','데이타마닝'))\n",
        "\n",
        "## 값이 이상함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM1aB7MDq2rJ",
        "outputId": "33a6d017-c0fc-4c7e-b1bc-0efb81dfc7ce"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 2. 3. 4. 5.]\n",
            " [1. 0. 1. 2. 3. 4.]\n",
            " [2. 0. 0. 0. 0. 0.]\n",
            " [3. 0. 0. 0. 0. 0.]\n",
            " [4. 0. 0. 0. 0. 0.]\n",
            " [5. 0. 0. 0. 0. 0.]\n",
            " [6. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbuu2UqYsLGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}